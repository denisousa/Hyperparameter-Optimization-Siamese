##### GENERAL CONFIGURATIONS
# location of elasticsearch
elasticsearchLoc=/Users/Chaiyong/Documents/phd/2017/Siamese/elasticsearch-2.2.0
#elasticsearchLoc=/home/cragkhit/data/elasticsearch-2.2.0
# elasticsearch's server name (or IP)
server=localhost
# elasticsearch's cluster name
cluster=stackoverflow
# index name
index=bellon7
# type name
type=siamese
# input folder of source files to search
### For CloPlag data set
#inputFolder=/Users/Chaiyong/Documents/phd/2017/BellonClonesReader/queries_t1/
inputFolder=/Users/Chaiyong/Desktop/temp
### For SOCO data set
# inputFolder=/Users/Chaiyong/Documents/phd/2016/cloplag/soco_f/formatted/
### For BCB data set
#inputFolder=/Users/Chaiyong/Downloads/dataset/
# only for GitHub indexing
subInputFolder=
# output folder to store the search results
outputFolder=exp_results
# normalisation.
# It can be a combination of x (none), w (words), d (datatypes),
# j (Java classes), p (Java packages), k (keywords), v (values),
# s (strings), o (operators), e (escape). For example: wkvs
#normMode=djkopsvw
normMode=x
# turn on ngram
isNgram=true
# size of n in ngram [default = 14]
ngramSize=7
t2NgramSize=16
t1NgramSize=4
# use DFS mode [default=no]
dfs=true
writeToFile=true
# source code file extension
extension=java
# minimum clone size (lines)
minCloneSize=1
# command to execute [index,search]
command=search
# adding new documents by deleting the existing ones [true, false]
recreateIndexIfExists=true
# print out logging data
isPrint=false
# output format [csv = filename, csvfline = filename#start#end, gcf = XML)]
outputFormat=csvfline
# indexing mode [sequential, bulk]
indexingMode=bulk
# size of bulk insert
bulkSize=4000

##### DELETE SETTINGS
deleteField=
deleteWildcard=
deleteAmount=1000

##### LANGUAGE SETTINGS
methodParser=crest.siamese.language.java.JavaMethodParser
tokenizer=crest.siamese.language.java.JavaTokenizer
normalizer=crest.siamese.language.java.JavaNormalizer

##### MULTI-REPRESENTATION CONFIGURATIONS
multirep=false
#enableRep=true,true,true,true
enableRep=true,true,true,true

##### QUERY CONFIGURATIONS
resultOffset=0
resultsSize=100
totalDocuments=100
# method, file
parseMode=file
printEvery=10000
# tfidf, bm25, dfr, ib, lmd (LM Dirichlet), lmj (LM Jelinek-Mercer)
rankingFunction=tfidf

##### QUERY REDUCTION SETTINGS
# turn on query reduction [true/false]
#queryReduction=true
queryReduction=false
# reduction percentile for the T3 layer [25,50,75]
QRPercentileNorm=10
# reduction percentile for the T2 layer [25,50,75]
QRPercentileT2=10
# reduction percentile for the T1 layer [25,50,75]
QRPercentileT1=10
# reduction percentile for the T1 layer [25,50,75]
QRPercentileOrig=10
# boosting for T3 layer
normBoost=6
# boosting for T2 layer
t2Boost=16
# boosting for T1 layer
t1Boost=4
# boosting for T0 layer
origBoost=1

##### EXPERIMENT CONFIGURATIONS
# elasticsearch similarity function + ngram + normalisation [or both]
similarityMode=tfidf_text_def
# prefix of the clone cluster file name [cloplag/soco]
cloneClusterFile=soco
# IR error measure [arp/map]
errorMeasure=map
# delete the index after every run?
deleteIndexAfterUse=true

##### LICENSE EXTRACTION
# extract license [true, false]
license=true
# license extractor [ninka, regexp]
licenseExtractor=regexp

##### SIMILARITY
# compute similarity of the results [fuzzywuzzy, tokenratio, none]
computeSimilarity=none
# the similarity threshold for the four representations [T1,T2,T3,T4] respectively
simThreshold=50%,50%,50%,50%
# GitHub indexing? (automatically add URL)
github=false
